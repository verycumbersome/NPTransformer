{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 256\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using dim 256 instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'the cat likes oranges', 'target': 'le chat aime les oranges', 'input_embedding': array([[ 0.16699061, -0.1185919 ,  0.02268532, ...,  0.05452403,\n",
      "         0.00290791, -0.0578087 ],\n",
      "       [ 0.01792765, -0.1697452 , -0.2524293 , ...,  0.02038066,\n",
      "        -0.03436632,  0.03158564],\n",
      "       [ 0.1284513 , -0.01115857, -0.10007418, ...,  0.0344702 ,\n",
      "        -0.01029698,  0.02521799],\n",
      "       [-0.0449486 , -0.11437774, -0.08717595, ...,  0.00913693,\n",
      "        -0.08598089, -0.07585137]], dtype=float32), 'target_embedding': array([[ 0.10709047, -0.5281988 ,  0.01931942, ...,  0.0100735 ,\n",
      "         0.0801144 , -0.14083575],\n",
      "       [-0.6774422 , -0.5310623 ,  0.30984998, ...,  0.35692137,\n",
      "         0.05885062, -0.02483021],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ...,\n",
      "       [-1.1296873 , -0.97862786,  0.44244942, ...,  0.15799554,\n",
      "         0.14931174, -0.18530309],\n",
      "       [-0.6774422 , -0.5310623 ,  0.30984998, ...,  0.35692137,\n",
      "         0.05885062, -0.02483021],\n",
      "       [-0.19802126, -0.5308491 ,  0.63903207, ..., -0.0025325 ,\n",
      "        -0.23603661, -0.27881238]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset():\n",
    "    def __init__(self, inputs, targets, embeddings):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        en_i = embeddings[\"en\"]\n",
    "        en_t = embeddings[\"fr\"]\n",
    "        \n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            self.input_em.append(np.array([en_i.get_word_vector(w) for w in seq.split()]))\n",
    "            \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            self.target_em.append(np.array([en_t.get_word_vector(w) for w in seq]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "\n",
    "dataset = TranslationDataset(data, target, embeddings) \n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat likes oranges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.16699061,  0.42171041, -0.39346152, ..., -0.04709166,\n",
       "        -0.88911059, -0.92011231],\n",
       "       [ 0.01792765,  0.63221659,  0.70571507, ...,  0.20405458,\n",
       "        -0.71296226, -0.96284121],\n",
       "       [ 1.1284513 ,  0.6367473 , -0.26051014, ...,  0.71485267,\n",
       "         0.98875911,  0.63942415],\n",
       "       [-0.0449486 ,  0.60703638,  0.91198825, ...,  0.32887116,\n",
       "        -0.54810881, -1.03563637]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)\n",
    "    \n",
    "\n",
    "print(dataset[0][\"input\"])\n",
    "\n",
    "pos_encoding(dataset[0][\"input_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.03234729, 0.03117249, 0.03143047, ..., 0.03085842, 0.03111427,\n",
       "         0.03122871],\n",
       "        [0.03210062, 0.03099027, 0.03126996, ..., 0.03118081, 0.031097  ,\n",
       "         0.0312079 ],\n",
       "        [0.03250291, 0.031115  , 0.03143878, ..., 0.03082216, 0.03106583,\n",
       "         0.03123561],\n",
       "        ...,\n",
       "        [0.0312418 , 0.03117179, 0.03121468, ..., 0.03137008, 0.03119591,\n",
       "         0.03115347],\n",
       "        [0.03016306, 0.0312986 , 0.03098123, ..., 0.03164659, 0.03156188,\n",
       "         0.03156694],\n",
       "        [0.03179544, 0.03113163, 0.03132864, ..., 0.03121022, 0.03106063,\n",
       "         0.03100564]]),\n",
       " array([[ 1.16035980e-01, -1.11178042e-01, -1.54323736e-02,\n",
       "         -1.08331929e-01],\n",
       "        [ 1.28830832e-01, -1.25255464e-01,  1.75648130e-01,\n",
       "          4.66058847e-02],\n",
       "        [-1.04599617e-01,  8.98262042e-03, -1.44877619e-02,\n",
       "          3.65305412e-03],\n",
       "        [-1.23249278e-02, -2.32757982e-01, -8.53251355e-02,\n",
       "         -4.04423026e-04],\n",
       "        [-1.17240787e-01, -2.29391950e-01, -1.18472205e-01,\n",
       "         -6.09918955e-02],\n",
       "        [ 5.99499758e-02, -1.09565031e-01, -3.69720446e-02,\n",
       "         -4.49191478e-03],\n",
       "        [ 2.40334578e-01, -7.04007936e-02, -5.53619723e-02,\n",
       "          3.00882317e-02],\n",
       "        [ 7.18062574e-02,  1.21740772e-01,  7.70965279e-02,\n",
       "         -4.30704335e-02],\n",
       "        [ 1.01747004e-01,  1.60121794e-01, -8.67931624e-02,\n",
       "         -3.14630163e-02],\n",
       "        [-1.32233275e-02,  3.74894154e-01,  6.06061079e-03,\n",
       "         -2.95872909e-02],\n",
       "        [-3.51141785e-02, -3.90947755e-01, -1.35929498e-02,\n",
       "          1.19343540e-01],\n",
       "        [ 1.84000334e-01,  2.71716971e-01,  9.14942946e-02,\n",
       "          1.19650191e-01],\n",
       "        [-5.10525936e-02, -1.67264762e-01, -4.52773962e-02,\n",
       "         -2.95090370e-02],\n",
       "        [-3.92748846e-02, -1.21449645e-01,  1.43698477e-03,\n",
       "         -1.51509074e-01],\n",
       "        [-1.02606037e-01, -1.34016144e-01,  5.86683977e-03,\n",
       "         -8.10387757e-02],\n",
       "        [ 2.35166957e-01,  3.72189452e-01,  8.63120457e-02,\n",
       "          9.23307155e-02],\n",
       "        [ 1.06760178e-01,  2.32051296e-01,  7.71133275e-02,\n",
       "          5.47248187e-02],\n",
       "        [-2.35477308e-02,  4.18020825e-01,  1.08709830e-01,\n",
       "          2.56051997e-02],\n",
       "        [ 1.02856574e-01,  7.50175255e-03,  6.71503407e-02,\n",
       "         -1.64440181e-01],\n",
       "        [ 4.01305938e-02,  2.26690068e-01,  3.42558534e-02,\n",
       "          9.09580576e-03],\n",
       "        [ 1.69306375e-01,  4.25213523e-02, -2.72446981e-03,\n",
       "          4.00738895e-03],\n",
       "        [-1.66059886e-01, -7.25761227e-02,  2.64180731e-03,\n",
       "         -7.18372603e-02],\n",
       "        [-3.58879931e-02,  1.37473569e-02, -1.16062140e-02,\n",
       "         -1.07566678e-01],\n",
       "        [-1.08525486e-01, -1.65617757e-01,  9.51872797e-02,\n",
       "          5.22589724e-02],\n",
       "        [ 3.54885758e-03,  8.42687086e-02, -8.68734601e-03,\n",
       "          2.92789502e-02],\n",
       "        [-1.79164574e-01, -7.89809393e-02,  1.27342285e-02,\n",
       "          9.43994159e-02],\n",
       "        [ 1.65542527e-01, -7.68485994e-02, -3.62123476e-02,\n",
       "          6.10196869e-02],\n",
       "        [-1.27270880e-01,  4.62069750e-01,  2.12232395e-01,\n",
       "          1.29892234e-01],\n",
       "        [ 3.46280775e-01,  1.84466986e-01, -1.14426151e-01,\n",
       "          5.80045923e-02],\n",
       "        [-3.39683183e-02, -5.25562951e-02,  1.77581622e-02,\n",
       "          6.55622035e-03],\n",
       "        [-5.39419363e-02, -4.25872713e-02,  1.17468360e-01,\n",
       "         -2.04354271e-02],\n",
       "        [ 1.60303484e-01,  3.58215756e-01,  1.11321068e-01,\n",
       "         -2.78631079e-02]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, num_heads=NUM_HEADS):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads \n",
    "        self.output_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.V = nn.LinearLayer(embed_dim, output_dim)\n",
    "        self.K = nn.LinearLayer(embed_dim, output_dim)\n",
    "        self.Q = nn.LinearLayer(embed_dim, output_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Attention forward pass\"\"\"\n",
    "        d_k = self.embed_dim // self.output_dim\n",
    "        scale = math.sqrt(d_k)\n",
    "        \n",
    "        return(nn.softmax(np.dot(self.Q(x), self.K(x).T) / scale), self.V(x))\n",
    "        \n",
    "        \n",
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "\n",
    "head = AttentionHead()\n",
    "head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
