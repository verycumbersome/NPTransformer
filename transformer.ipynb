{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "#np.seterr(divide='raise', invalid='raise')\n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 64\n",
    "INNER_DIM = MODEL_DIM * 4\n",
    "\n",
    "SEQ_LEN = 128 # Max length of input or output sentence\n",
    "NUM_HEADS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using MODEL_DIM instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/english.pkl\n",
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Please rise, then, for this minute' s silence.\n",
      "(The House rose and observed a minute' s silence)\n",
      "Madam President, on a point of order.\n",
      "You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "Saved: data/french.pkl\n",
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
      "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
      "Je vous invite à vous lever pour cette minute de silence.\n",
      "(Le Parlement, debout, observe une minute de silence)\n",
      "Madame la Présidente, c'est une motion de procédure.\n",
      "Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.\n",
      "L'une des personnes qui vient d'être assassinée au Sri Lanka est M. Kumar Ponnambalam, qui avait rendu visite au Parlement européen il y a quelques mois à peine.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2a1ba8034fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/english.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/french.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        line = line.split()\n",
    "        line = [word.lower() for word in line]\n",
    "        line = [word.translate(table) for word in line]\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    " \n",
    "# load English data\n",
    "filename = 'data/europarl-v7.fr-en.en'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "english_sentences = clean_lines(sentences[:10000])\n",
    "save_clean_sentences(english_sentences, 'data/english.pkl')\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    " \n",
    "# load French data\n",
    "filename = 'data/europarl-v7.fr-en.fr'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "french_sentences = clean_lines(sentences[:10000])\n",
    "save_clean_sentences(french_sentences, 'data/french.pkl')\n",
    "\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    "    \n",
    "\n",
    "with open('data/english.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "with open('data/french.pkl', 'rb') as f:\n",
    "    target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_embedding(x, pad_len, embed_dim):\n",
    "    \"\"\"Pads a word embedding to a max length\"\"\"\n",
    "    try:\n",
    "        padded = np.zeros((pad_len - x.shape[0], embed_dim))\n",
    "        return np.concatenate((x, padded))\n",
    "    except:\n",
    "        print(pad_len)\n",
    "        print(x.shape)\n",
    "        print(x is None)\n",
    "\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"Dataset for the position encoded and word embedded translations\"\"\"\n",
    "    def __init__(self, inputs, targets, embeddings, pad_len, embed_dim):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        emb_in = embeddings[\"en\"]\n",
    "        emb_tgt = embeddings[\"fr\"]\n",
    "        \n",
    "        # Embed all inputs\n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            embed = np.array([emb_in.get_word_vector(w) for w in seq.split()])\n",
    "            if embed.any() and embed.shape[0] < SEQ_LEN:\n",
    "                embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "                self.input_em.append(np.array(embed))\n",
    "            \n",
    "        # Embed all outputs \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            embed = np.array([emb_tgt.get_word_vector(w) for w in seq.split()])\n",
    "            if embed.any() and embed.shape[0] < SEQ_LEN:\n",
    "                embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "                self.target_em.append(np.array(embed))\n",
    "    \n",
    "    def pad_embedding(self, x):\n",
    "        padded = np.zeros((self.pad_length - x.shape[0], self.embed_dim))\n",
    "        return np.concatenate((x, padded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "\n",
    "import pickle \n",
    "with open('data/english.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "with open('data/french.pkl', 'rb') as f:\n",
    "    target = pickle.load(f)\n",
    "    \n",
    "data = english_sentences\n",
    "target = french_sentences\n",
    "\n",
    "#data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "#target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "#\n",
    "dataset = TranslationDataset(data, target, embeddings, SEQ_LEN, MODEL_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ee721e1f01b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = embed_dim // n_heads\n",
    "        #self.dim_k = math.sqrt(self.embed_dim // self.output_dim)\n",
    "        self.dim_k = math.sqrt(self.output_dim)\n",
    "        \n",
    "        self.layers = [np.random.randn(self.output_dim, embed_dim) * \\\n",
    "                                 np.sqrt(2 / self.embed_dim) for _ in range(3)]\n",
    "        \n",
    "        #TODO find clean alternative\n",
    "        self.V, self.K, self.Q = self.layers[0], self.layers[1], self.layers[2]\n",
    "        \n",
    "        # Useful for calculating gradient\n",
    "        self.ones = (np.ones((SEQ_LEN, self.output_dim)))\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Attention forward pass\n",
    "        Function: softmax(QK^T/sqrt(dim_k) * V)\n",
    "        \"\"\"\n",
    "        # Scaled dot product\n",
    "        scaled = np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k\n",
    "        \n",
    "        # Masking (Optional)\n",
    "        if mask is not None:\n",
    "            scaled = np.add(mask, scaled)\n",
    "            \n",
    "        self.layer_output = np.matmul(nn.softmax(scaled), np.dot(self.V, x.T)).T\n",
    "        self.layer_output = np.nan_to_num(self.layer_output)\n",
    "        return(self.layer_output)\n",
    "    \n",
    "    def F(self, x):\n",
    "        \"\"\"KQ^T/d_k for backprop\"\"\"\n",
    "        return(np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k)\n",
    "    \n",
    "    def step(self, x, t, alpha=0.0001, eps=1e-9):\n",
    "        \"\"\"Step along gradients to update K, Q, V weights.\"\"\"\n",
    "        # Softmax derivative\n",
    "        dS = np.matmul(nn.softmax(self.F(x)), \n",
    "                       (np.identity(self.output_dim) - nn.softmax(self.F(x))))\n",
    "        \n",
    "        # Cross entropy derivative\n",
    "        dL = np.divide(-t, self.layer_output)\n",
    "        dL = dL + ((self.ones - t) / (self.ones - self.layer_output))\n",
    "        dL[np.isnan(dL)] = 0\n",
    "        \n",
    "        # Calculate loss w.r.t weight gradients\n",
    "        # dL/dK\n",
    "        dK = np.matmul(self.Q, x.T)\n",
    "        dK = np.matmul(dK, x).T\n",
    "        dK = np.diag(1 / self.dim_k * np.kron(dK, np.identity(self.output_dim)))\n",
    "        \n",
    "        # dL/dQ\n",
    "        dQ = np.matmul(self.K, np.matmul(x.T, x))\n",
    "        dQ = np.kron(np.identity(self.output_dim), self.K)\n",
    "        dQ = np.diag(1 / self.dim_k * dQ)\n",
    "        \n",
    "        # dL/dV\n",
    "        # TODO\n",
    "        \n",
    "        # Calc deltas and update weights\n",
    "        del_K = np.nan_to_num(np.matmul(dL, np.nan_to_num(dS)))\n",
    "        self.K -= np.einsum(\"ki,j->ij\", del_K, dK) * alpha\n",
    "        \n",
    "        del_Q = np.nan_to_num(np.matmul(dL, dS))\n",
    "        self.Q -= np.einsum(\"ki,j->ij\", del_Q, dQ) * alpha\n",
    "\n",
    "        \n",
    "x = dataset[2][\"input_embedding\"]\n",
    "t = dataset[2][\"input_embedding\"]\n",
    "\n",
    "head = AttentionHead(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "t = t[:,0:8]\n",
    "\n",
    "output = head(x)\n",
    "print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "for _ in range(1000):\n",
    "    head.step(x, t[:,0:8])\n",
    "    output = head(x)\n",
    "    outputs.append(nn.cross_entropy(output, t))\n",
    "    \n",
    "plt.plot(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 seq_len=SEQ_LEN, masked=False):\n",
    "        # Class vars\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attn_dim = embed_dim // n_heads\n",
    "        \n",
    "        # Create all heads and weights for multiheaded attention\n",
    "        self.heads = [AttentionHead(embed_dim, n_heads) for _ in range(n_heads)]\n",
    "        self.O = np.random.randn(embed_dim, embed_dim) * math.sqrt(2 / (embed_dim * seq_len))\n",
    "        \n",
    "        # If the attention block is masked\n",
    "        self.mask = None\n",
    "        if masked:\n",
    "            self.mask = np.ones((self.attn_dim, self.attn_dim)) * -np.inf\n",
    "            self.mask = np.triu(self.mask, k=1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward passes through concatenated heads and matmuls by weights\"\"\"\n",
    "        h_cat = np.concatenate([h(x, self.mask) for h in self.heads], axis=1)\n",
    "        self.layer_output = np.nan_to_num(np.matmul(h_cat, self.O))\n",
    "        return(self.layer_output)\n",
    "        \n",
    "    def backprop(self, pred, target, alpha=0.00001):\n",
    "        \"\"\"Makes one step along each gradient of each attention head\"\"\"\n",
    "        # Derivative of layer given output from multihead attention\n",
    "        delta = -(target / pred)\n",
    "        delta[np.isnan(delta)] = 0\n",
    "        delta = (np.ones(target.shape) - target) / (np.ones(pred.shape) - pred)\n",
    "        delta[np.isnan(delta)] = 0\n",
    "        self.O -= np.matmul(delta.T, pred).T * alpha\n",
    "        \n",
    "        target = self.__call__(pred)\n",
    "        \n",
    "        # TODO Fix this\n",
    "        for i, h in enumerate(self.heads):\n",
    "            t = target[:,0 + i * self.attn_dim:(1 + i) * self.attn_dim] #0-32, 33-64, etc.\n",
    "            h.step(pred, t, alpha)\n",
    "        \n",
    "       # # Derivative of layer given output from multihead attention\n",
    "       # pred = self.layer_output\n",
    "       # delta = -(target / pred)\n",
    "       # delta[np.isnan(delta)] = 0\n",
    "       # delta = (np.ones(target.shape) - target) / (np.ones(pred.shape) - pred)\n",
    "       # delta[np.isnan(delta)] = 0\n",
    "       # self.O -= np.matmul(delta.T, pred).T * alpha\n",
    "        \n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "attn = MultiheadAttention(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "output = attn(x)\n",
    "#print(output.shape)\n",
    "#print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "for i in range(1000):\n",
    "    #x = dataset[i][\"input_embedding\"]\n",
    "    #t = dataset[i][\"target_embedding\"]\n",
    "    attn.backprop(x, t)\n",
    "    output = attn(x)\n",
    "    loss = nn.cross_entropy(output, t)\n",
    "    outputs.append(loss)\n",
    "    \n",
    "plt.plot(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEQUENCE_MAX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f0ae2044631f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFFN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Position-wise feed forward nueral network\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINNER_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQUENCE_MAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f0ae2044631f>\u001b[0m in \u001b[0;36mFFN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFFN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Position-wise feed forward nueral network\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINNER_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQUENCE_MAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SEQUENCE_MAX' is not defined"
     ]
    }
   ],
   "source": [
    "class FFN(nn.Net):\n",
    "    \"\"\"Position-wise feed forward nueral network\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        super(FFN, self).__init__()\n",
    "        self.L1 = nn.LinearLayer(embed_dim, inner_dim, sequence_max)\n",
    "        self.L2 = nn.LinearLayer(inner_dim, embed_dim, sequence_max)\n",
    "        \n",
    "        self.layers = [\n",
    "            self.L1,\n",
    "            self.L2,\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "        x = self.L2(np.maximum(self.L1(x), 0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement layer norm\n",
    "def layer_norm(layer):\n",
    "    std = np.std(layer)\n",
    "    mean = np.mean(layer)\n",
    "    print(std, mean)\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    \n",
    "    \"\"\"Encoder block for the transformer.\n",
    "    Args:\n",
    "            embed_dim (string): Directory with all the images.\n",
    "            n_heads (string): Path to the csv file with annotations.\n",
    "            sequence_max (int):\n",
    "            inner_dim (int):\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 inner_dim=INNER_DIM, sequence_max=SEQ_LEN):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.seq_len = seq_len \n",
    "        \n",
    "        self.multihead_attn = MultiheadAttention(embed_dim, n_heads)\n",
    "        self.feedforward = FFN(embed_dim, inner_dim, seq_len)\n",
    "    \n",
    "    def temp_add_norm(self, x, y):\n",
    "        # TODO replace with layer norm function\n",
    "        x = np.add(x, y) \n",
    "        return (x / np.sqrt(np.sum(x**2))).T\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = self.multihead_attn(x)\n",
    "        x = self.temp_add_norm(x, y.T)\n",
    "        \n",
    "        y = self.feedforward(x)\n",
    "        x = self.temp_add_norm(x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#multihead_attn = MultiheadAttention()\n",
    "#ffn = FFN()\n",
    "#\n",
    "#x = dataset[0][\"input_embedding\"]\n",
    "#t = dataset[0][\"input_embedding\"]\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "#\n",
    "##multihead_attn.backprop(x, t)\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "\n",
    "\n",
    "#x = ffn(t.T)\n",
    "#\n",
    "#\n",
    "#ffn.backprop(x, t)\n",
    "#print(nn.loss(x, t))\n",
    "\n",
    "#for i in range(x.shape[1]):\n",
    "#    print(x[:,i].shape, t.T[:,i].shape)\n",
    "#    ffn.backprop(x[:,i], t.T[:,i])\n",
    "#    print(nn.loss(x, t.T))\n",
    "    \n",
    "\n",
    "#layer_norm(x)\n",
    "#enc_block = EncoderBlock()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
