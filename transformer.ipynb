{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 256\n",
    "INNER_DIM = 1024 \n",
    "\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using dim 256 instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [],
   "source": [
    "class TranslationDataset():\n",
    "    def __init__(self, inputs, targets, embeddings):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        en_i = embeddings[\"en\"]\n",
    "        en_t = embeddings[\"fr\"]\n",
    "        \n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            self.input_em.append(np.array([en_i.get_word_vector(w) for w in seq.split()]))\n",
    "            \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            self.target_em.append(np.array([en_t.get_word_vector(w) for w in seq]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "\n",
    "dataset = TranslationDataset(data, target, embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.73479643e-03 -2.81575693e-02  6.38276504e-02  1.12585503e-02]\n",
      " [-7.80738386e-03  1.33358979e-01  2.14657868e-02 -2.85409773e-02]\n",
      " [ 8.26628697e-03  3.91261445e-02 -1.40601615e-02  1.20711907e-02]\n",
      " ...\n",
      " [-6.68998012e-02 -1.54699273e-02 -4.40439950e-02  8.19583891e-05]\n",
      " [-5.27294279e-02  5.81298962e-02 -3.27853882e-02  2.88939011e-03]\n",
      " [-3.31982013e-02  4.41935133e-02 -7.06124802e-02  1.82914396e-02]]\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.output_dim = embed_dim // n_heads\n",
    "        \n",
    "        self.V = nn.LinearLayer(embed_dim, self.output_dim)\n",
    "        self.K = nn.LinearLayer(embed_dim, self.output_dim)\n",
    "        self.Q = nn.LinearLayer(embed_dim, self.output_dim)\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Attention forward pass\"\"\"\n",
    "        d_k = self.embed_dim // self.output_dim\n",
    "        scale = math.sqrt(d_k)\n",
    "        \n",
    "        scaled_dp = np.dot(self.Q(x.T), self.K(x.T).T) / scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_dp = np.add(mask, scaled_dp)\n",
    "            \n",
    "        return(np.matmul(nn.softmax(scaled_dp), self.V(x.T)))\n",
    "        \n",
    "        \n",
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, masked=False):\n",
    "        self.heads = [AttentionHead(embed_dim, n_heads) for _ in range(n_heads)]\n",
    "        self.O = nn.LinearLayer(embed_dim, embed_dim)\n",
    "        \n",
    "        self.attn_dim = embed_dim // n_heads\n",
    "        self.masked = masked\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.masked:\n",
    "            mask = np.triu(np.ones((self.attn_dim, self.attn_dim)) * -np.inf, k=1)\n",
    "            \n",
    "        head_sum = np.concatenate(np.array([h(x, mask) for h in self.heads]))\n",
    "        \n",
    "        return(self.O(head_sum))\n",
    "\n",
    "x = dataset[0][\"input_embedding\"]\n",
    "\n",
    "multihead_attn = MultiheadAttention(masked=True)\n",
    "\n",
    "print(multihead_attn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Net):\n",
    "    \"\"\"Position-wise feed forward nueral network\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, inner_dim=INNER_DIM):\n",
    "        self.L1 = nn.LinearLayer(embed_dim, inner_dim)\n",
    "        self.L2 = nn.LinearLayer(inner_dim, embed_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "        x = self.L2(np.maximum(self.L1(x), 0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'mask' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-2d93ee630332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmultihead_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultihead_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-527e8e86743d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mhead_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-527e8e86743d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mhead_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-527e8e86743d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'mask' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#TODO implement layer norm\n",
    "def layer_norm(layer):\n",
    "    std = np.std(layer)\n",
    "    mean = np.mean(layer)\n",
    "    print(std, mean)\n",
    "    print(layer)\n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "\n",
    "multihead_attn = MultiheadAttention()\n",
    "ffn = FFN()\n",
    "x = multihead_attn(x)\n",
    "\n",
    "x = ffn(x)\n",
    "\n",
    "layer_norm(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderBlock():\n",
    "    def __init__(self):\n",
    "        \n",
    "    def __call__(self, x):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
