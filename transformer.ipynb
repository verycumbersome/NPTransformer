{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 64\n",
    "INNER_DIM = MODEL_DIM * 4\n",
    "\n",
    "SEQUENCE_MAX = 128 # Max length of input or output sentence\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using dim 256 instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_embedding(x, pad_len, embed_dim):\n",
    "    \"\"\"Pads a word embedding to a max length\"\"\"\n",
    "    padded = np.zeros((pad_len - x.shape[0], embed_dim))\n",
    "    return np.concatenate((x, padded))\n",
    "\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"Dataset for the position encoded and word embedded translations\"\"\"\n",
    "    def __init__(self, inputs, targets, embeddings, pad_len, embed_dim):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        emb_in = embeddings[\"en\"]\n",
    "        emb_tgt = embeddings[\"fr\"]\n",
    "        \n",
    "        # Embed all inputs\n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            embed = np.array([emb_in.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.input_em.append(np.array(embed))\n",
    "            \n",
    "        # Embed all outputs \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            embed = np.array([emb_tgt.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.target_em.append(np.array(embed))\n",
    "    \n",
    "    def pad_embedding(self, x):\n",
    "        padded = np.zeros((self.pad_length - x.shape[0], self.embed_dim))\n",
    "        return np.concatenate((x, padded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "\n",
    "dataset = TranslationDataset(data, target, embeddings, SEQUENCE_MAX, MODEL_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.37731713237133\n",
      "156.3774347092205\n",
      "156.3775525441898\n",
      "156.37767063717737\n",
      "156.37778898808142\n",
      "156.37790759680001\n",
      "156.37802646323144\n",
      "156.3781455872737\n",
      "156.37826496882508\n",
      "156.37838460778374\n",
      "156.37850450404778\n",
      "156.3786246575154\n",
      "156.37874506808475\n",
      "156.37886573565413\n",
      "156.3789866601216\n",
      "156.37910784138546\n",
      "156.37922927934386\n",
      "156.37935097389504\n",
      "156.37947292493726\n",
      "156.3795951323688\n",
      "156.37971759608786\n",
      "156.37984031599274\n",
      "156.37996329198177\n",
      "156.38008652395325\n",
      "156.3802100118055\n",
      "156.3803337554369\n",
      "156.3804577547458\n",
      "156.38058200963061\n",
      "156.38070651998976\n",
      "156.38083128572163\n",
      "156.38095630672478\n",
      "156.38108158289762\n",
      "156.3812071141387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-546-ae715bd9d06d>:44: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dL = (-t / self.layer_output) + ((ones - t) / (ones - self.layer_output))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.3813329003466\n",
      "156.38145894141985\n",
      "156.38158523725707\n",
      "156.38171178775693\n",
      "156.381838592818\n",
      "156.3819656523391\n",
      "156.38209296621886\n",
      "156.38222053435612\n",
      "156.38234835664963\n",
      "156.38247643299826\n",
      "156.3826047633009\n",
      "156.38273334745645\n",
      "156.38286218536382\n",
      "156.3829912769221\n",
      "156.38312062203022\n",
      "156.38325022058734\n",
      "156.38338007249257\n",
      "156.38351017764506\n",
      "156.38364053594404\n",
      "156.38377114728874\n",
      "156.38390201157847\n",
      "156.38403312871264\n",
      "156.38416449859056\n",
      "156.3842961211118\n",
      "156.38442799617576\n",
      "156.38456012368204\n",
      "156.38469250353023\n",
      "156.38482513562002\n",
      "156.38495801985118\n",
      "156.3850911561234\n",
      "156.38522454433655\n",
      "156.3853581843905\n",
      "156.3854920761852\n",
      "156.3856262196207\n",
      "156.38576061459705\n",
      "156.38589526101435\n",
      "156.38603015877285\n",
      "156.38616530777279\n",
      "156.38630070791444\n",
      "156.38643635909827\n",
      "156.38657226122467\n",
      "156.38670841419423\n",
      "156.38684481790744\n",
      "156.38698147226506\n",
      "156.38711837716778\n",
      "156.38725553251643\n",
      "156.38739293821183\n",
      "156.38753059415495\n",
      "156.38766850024683\n",
      "156.38780665638853\n",
      "156.38794506248126\n",
      "156.38808371842623\n",
      "156.3882226241248\n",
      "156.38836177947837\n",
      "156.38850118438836\n",
      "156.38864083875643\n",
      "156.3887807424842\n",
      "156.38892089547335\n",
      "156.38906129762574\n",
      "156.38920194884327\n",
      "156.38934284902786\n",
      "156.38948399808163\n",
      "156.3896253959067\n",
      "156.38976704240537\n",
      "156.3899089374799\n",
      "156.39005108103277\n",
      "156.39019347296642\n",
      "156.39033611318354\n",
      "156.3904790015867\n",
      "156.3906221380788\n",
      "156.3907655225627\n",
      "156.3909091549413\n",
      "156.3910530351178\n",
      "156.39119716299524\n",
      "156.39134153847698\n",
      "156.39148616146633\n",
      "156.39163103186678\n",
      "156.39177614958186\n",
      "156.39192151451527\n",
      "156.39206712657074\n",
      "156.39221298565224\n",
      "156.3923590916636\n",
      "156.39250544450903\n",
      "156.39265204409264\n",
      "156.39279889031872\n",
      "156.3929459830917\n",
      "156.39309332231608\n",
      "156.39324090789648\n",
      "156.39338873973765\n",
      "156.39353681774435\n",
      "156.3936851418216\n",
      "156.39383371187444\n",
      "156.39398252780802\n",
      "156.39413158952766\n",
      "156.39428089693877\n",
      "156.39443044994687\n",
      "156.3945802484576\n",
      "156.39473029237666\n",
      "156.39488058161004\n",
      "156.39503111606362\n",
      "156.3951818956436\n",
      "156.39533292025612\n",
      "156.39548418980763\n",
      "156.3956357042046\n",
      "156.3957874633536\n",
      "156.39593946716138\n",
      "156.3960917155348\n",
      "156.39624420838084\n",
      "156.39639694560657\n",
      "156.39654992711928\n",
      "156.39670315282632\n",
      "156.3968566226352\n",
      "156.39701033645355\n",
      "156.3971642941891\n",
      "156.39731849574974\n",
      "156.39747294104353\n",
      "156.39762762997862\n",
      "156.39778256246325\n",
      "156.39793773840591\n",
      "156.3980931577152\n",
      "156.39824882029973\n",
      "156.3984047260684\n",
      "156.39856087493018\n",
      "156.39871726679422\n",
      "156.3988739015697\n",
      "156.39903077916608\n",
      "156.39918789949292\n",
      "156.39934526245992\n",
      "156.39950286797682\n",
      "156.3996607159537\n",
      "156.3998188063007\n",
      "156.39997713892797\n",
      "156.40013571374598\n",
      "156.40029453066535\n",
      "156.40045358959674\n",
      "156.40061289045104\n",
      "156.40077243313928\n",
      "156.4009322175726\n",
      "156.40109224366233\n",
      "156.4012525113199\n",
      "156.40141302045703\n",
      "156.40157377098546\n",
      "156.4017347628171\n",
      "156.40189599586404\n",
      "156.40205747003859\n",
      "156.4022191852531\n",
      "156.4023811414202\n",
      "156.40254333845257\n",
      "156.40270577626308\n",
      "156.40286845476484\n",
      "156.40303137387104\n",
      "156.40319453349503\n",
      "156.40335793355035\n",
      "156.40352157395077\n",
      "156.40368545461007\n",
      "156.40384957544234\n",
      "156.4040139363618\n",
      "156.40417853728275\n",
      "156.40434337811976\n",
      "156.40450845878757\n",
      "156.404673779201\n",
      "156.40483933927516\n",
      "156.40500513892528\n",
      "156.40517117806667\n",
      "156.40533745661494\n",
      "156.40550397448584\n",
      "156.4056707315953\n",
      "156.40583772785936\n"
     ]
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = embed_dim // n_heads\n",
    "        self.dim_k = math.sqrt(self.embed_dim // self.output_dim)\n",
    "        \n",
    "        self.layers = [np.random.randn(self.output_dim, embed_dim) * \\\n",
    "                                 np.sqrt(2 / embed_dim) for _ in range(3)]\n",
    "        \n",
    "        #TODO find clean alternative\n",
    "        self.V, self.K, self.Q = self.layers[0], self.layers[1], self.layers[2]\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Attention forward pass\n",
    "        Function: softmax(QK^T/sqrt(dim_k) * V)\n",
    "        \"\"\"\n",
    "        # Scaled dot product\n",
    "        scaled = np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k\n",
    "        \n",
    "        # Masking (Optional)\n",
    "        if mask is not None:\n",
    "            scaled = np.add(mask, scaled)\n",
    "            \n",
    "        self.layer_output = np.matmul(nn.softmax(scaled), np.dot(self.V, x.T)).T\n",
    "        return(self.layer_output)\n",
    "    \n",
    "    def step(self, x, t, alpha=0.01):\n",
    "        # Attention function\n",
    "        F = lambda f: np.dot(np.dot(self.Q, f.T), \n",
    "                      np.dot(self.K, f.T).T) / self.dim_k\n",
    "        \n",
    "        # K partial derivative\n",
    "        dK = np.matmul(self.Q, x.T)\n",
    "        dK = np.matmul(dK, x).T\n",
    "        dK = np.diag(1 / self.dim_k * np.kron(dK, np.identity(self.K.shape[0])))\n",
    "        \n",
    "        # TODO REPLACE THE HARDCODED 8\n",
    "        # Softmax derivative\n",
    "        dS = np.matmul(nn.softmax(F(x)), (np.identity(8) - nn.softmax(F(x))))\n",
    "        \n",
    "        # Cross entropy derivative\n",
    "        ones = (np.ones(self.layer_output.shape))\n",
    "        dL = (-t / self.layer_output) + ((ones - t) / (ones - self.layer_output))\n",
    "        \n",
    "        del_K = np.nan_to_num(np.matmul(dL, dS))\n",
    "        del_K = np.einsum(\"ki,j->ij\", del_K, dK) * alpha\n",
    "        \n",
    "        self.K -= del_K\n",
    "        \n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "head = AttentionHead(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "t = t[:,0:8]\n",
    "\n",
    "output = head(x)\n",
    "print(nn.loss(output, t))\n",
    "\n",
    "#print(A.layers[0].weights)\n",
    "for _ in range(200):\n",
    "    head.step(x, t)\n",
    "    #print(A.layers[0].weights)\n",
    "\n",
    "    output = head(x)\n",
    "    print(nn.loss(output, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, masked=False):\n",
    "        # Class vars\n",
    "        self.attn_dim = embed_dim // n_heads\n",
    "        \n",
    "        # Create all heads and weights for multiheaded attention\n",
    "        self.heads = [AttentionHead(embed_dim, n_heads) for _ in range(n_heads)]\n",
    "        self.O = nn.LinearLayer(embed_dim, embed_dim)\n",
    "        \n",
    "        # If the attention block is masked\n",
    "        self.mask = None\n",
    "        if masked:\n",
    "            self.mask = np.ones((self.attn_dim, self.attn_dim)) * -np.inf\n",
    "            self.mask = np.triu(self.mask, k=1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward passes through concatenated heads and matmuls by weights\"\"\"\n",
    "        h_cat = np.concatenate(np.array([h(x, self.mask) for h in self.heads]))\n",
    "        return(self.O(h_cat).T)\n",
    "        \n",
    "    def backprop(self, x, t, alpha=0.001):\n",
    "        # Makes one pass across all concatonated heads of multiheaded attention\n",
    "        for i, h in enumerate(self.heads):\n",
    "            t = x.T[0 + i * self.attn_dim:(1 + i) * self.attn_dim] #0-32, 33-64, etc.\n",
    "            \n",
    "            for layer in h.layers:\n",
    "                layer.weights -= layer.delta(t).T * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Net):\n",
    "    \"\"\"Position-wise feed forward nueral network\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        super(FFN, self).__init__()\n",
    "        self.L1 = nn.LinearLayer(embed_dim, inner_dim, sequence_max)\n",
    "        self.L2 = nn.LinearLayer(inner_dim, embed_dim, sequence_max)\n",
    "        \n",
    "        self.layers = [\n",
    "            self.L1,\n",
    "            self.L2,\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "        x = self.L2(np.maximum(self.L1(x), 0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement layer norm\n",
    "def layer_norm(layer):\n",
    "    std = np.std(layer)\n",
    "    mean = np.mean(layer)\n",
    "    print(std, mean)\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    \n",
    "    \"\"\"Encoder block for the transformer.\n",
    "    Args:\n",
    "            embed_dim (string): Directory with all the images.\n",
    "            n_heads (string): Path to the csv file with annotations.\n",
    "            sequence_max (int):\n",
    "            inner_dim (int):\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.sequence_max = sequence_max\n",
    "        \n",
    "        self.multihead_attn = MultiheadAttention(embed_dim, n_heads)\n",
    "        self.feedforward = FFN(embed_dim, inner_dim, sequence_max)\n",
    "    \n",
    "    def temp_add_norm(self, x, y):\n",
    "        # TODO replace with layer norm function\n",
    "        x = np.add(x, y) \n",
    "        return (x / np.sqrt(np.sum(x**2))).T\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = self.multihead_attn(x)\n",
    "        x = self.temp_add_norm(x, y.T)\n",
    "        \n",
    "        y = self.feedforward(x)\n",
    "        x = self.temp_add_norm(x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#multihead_attn = MultiheadAttention()\n",
    "#ffn = FFN()\n",
    "#\n",
    "#x = dataset[0][\"input_embedding\"]\n",
    "#t = dataset[0][\"input_embedding\"]\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "#\n",
    "##multihead_attn.backprop(x, t)\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "\n",
    "\n",
    "#x = ffn(t.T)\n",
    "#\n",
    "#\n",
    "#ffn.backprop(x, t)\n",
    "#print(nn.loss(x, t))\n",
    "\n",
    "#for i in range(x.shape[1]):\n",
    "#    print(x[:,i].shape, t.T[:,i].shape)\n",
    "#    ffn.backprop(x[:,i], t.T[:,i])\n",
    "#    print(nn.loss(x, t.T))\n",
    "    \n",
    "\n",
    "#layer_norm(x)\n",
    "#enc_block = EncoderBlock()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
