{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 64\n",
    "INNER_DIM = MODEL_DIM * 4\n",
    "\n",
    "SEQUENCE_MAX = 128 # Max length of input or output sentence\n",
    "NUM_HEADS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using dim 256 instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_embedding(x, pad_len, embed_dim):\n",
    "    \"\"\"Pads a word embedding to a max length\"\"\"\n",
    "    padded = np.zeros((pad_len - x.shape[0], embed_dim))\n",
    "    return np.concatenate((x, padded))\n",
    "\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"Dataset for the position encoded and word embedded translations\"\"\"\n",
    "    def __init__(self, inputs, targets, embeddings, pad_len, embed_dim):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        emb_in = embeddings[\"en\"]\n",
    "        emb_tgt = embeddings[\"fr\"]\n",
    "        \n",
    "        # Embed all inputs\n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            embed = np.array([emb_in.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.input_em.append(np.array(embed))\n",
    "            \n",
    "        # Embed all outputs \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            embed = np.array([emb_tgt.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.target_em.append(np.array(embed))\n",
    "    \n",
    "    def pad_embedding(self, x):\n",
    "        padded = np.zeros((self.pad_length - x.shape[0], self.embed_dim))\n",
    "        return np.concatenate((x, padded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "\n",
    "dataset = TranslationDataset(data, target, embeddings, SEQUENCE_MAX, MODEL_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulae for gradients:\n",
    "* ### K partial derivative:\n",
    "    1 / d_k * (x * (x.T * Q)) ⊗ Identity\n",
    "\n",
    "* ### Q partial derivative:\n",
    "    1 / d_k * Identity ⊗ (k * x * x.T)\n",
    "\n",
    "* ### V partial derivative:\n",
    "    −(S⊤⋅(t⊘t0)⋅x⊤−S⊤⋅((vector(1)−t)⊘(vector(1)−t0))⋅x⊤)\n",
    "        where t0 = S * V * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.062396717742461055\n",
      "(128, 8)\n",
      "(128, 64)\n",
      "(8, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 64 is different from 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-88a42ad10a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#print(A.layers[0].weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-88a42ad10a9e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, t, alpha, eps)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 64 is different from 8)"
     ]
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = embed_dim // n_heads\n",
    "        self.dim_k = math.sqrt(self.embed_dim // self.output_dim)\n",
    "        \n",
    "        self.layers = [np.random.randn(self.output_dim, embed_dim) * \\\n",
    "                                 np.sqrt(2 / embed_dim) for _ in range(3)]\n",
    "        \n",
    "        #TODO find clean alternative\n",
    "        self.V, self.K, self.Q = self.layers[0], self.layers[1], self.layers[2]\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Attention forward pass\n",
    "        Function: softmax(QK^T/sqrt(dim_k) * V)\n",
    "        \"\"\"\n",
    "        # Scaled dot product\n",
    "        scaled = np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k\n",
    "        \n",
    "        # Masking (Optional)\n",
    "        if mask is not None:\n",
    "            scaled = np.add(mask, scaled)\n",
    "            \n",
    "        self.layer_output = np.matmul(nn.softmax(scaled), np.dot(self.V, x.T)).T\n",
    "        return(self.layer_output)\n",
    "    \n",
    "    def F(self, x):\n",
    "        \"\"\"KQ^T/d_k for backprop\"\"\"\n",
    "        return(np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k)\n",
    "    \n",
    "    def step(self, x, t, alpha=0.0001, eps=1e-9):\n",
    "        \"\"\"Step along gradients to update K, Q, V weights.\"\"\"\n",
    "        one = (np.ones(self.layer_output.shape))\n",
    "        \n",
    "        # Softmax derivative\n",
    "        dS = np.matmul(nn.softmax(self.F(x)), \n",
    "                       (np.identity(self.output_dim) - nn.softmax(self.F(x))))\n",
    "        \n",
    "        # Cross entropy derivative\n",
    "        dL = np.divide(-t, self.layer_output + eps) + \\\n",
    "             np.divide((one - t), (one - self.layer_output) + eps)\n",
    "        \n",
    "        # Calculate loss w.r.t weight gradients\n",
    "        # dL/dK\n",
    "        dK = np.matmul(self.Q, x.T)\n",
    "        dK = np.matmul(dK, x).T\n",
    "        dK = np.diag(1 / self.dim_k * np.kron(dK, np.identity(self.output_dim)))\n",
    "        \n",
    "        # dL/dQ\n",
    "        dQ = np.matmul(self.K, np.matmul(x.T, x))\n",
    "        dQ = np.kron(np.identity(self.output_dim), self.K)\n",
    "        dQ = np.diag(1 / self.dim_k * dQ)\n",
    "        \n",
    "        # dL/dV\n",
    "        \n",
    "        print(dL.shape)\n",
    "        print(x.shape)\n",
    "        print(nn.softmax(self.F(x)).shape)\n",
    "        dV = np.matmul(nn.softmax(self.F(x)), x.T)\n",
    "        print(dV.shape)\n",
    "        \n",
    "        dV = np.divide(-t, nn.softmax(self.F(x)) + eps) + \\\n",
    "             np.divide((one - t), (one - self.layer_output) + eps)\n",
    "        dV = dL\n",
    "        dV = np.matmul(x, dS)\n",
    "        print(dQ.shape)\n",
    "        print(dV.shape)\n",
    "        \n",
    "        # Calc deltas and update weights\n",
    "        del_K = np.nan_to_num(np.matmul(dL, np.nan_to_num(dS)))\n",
    "        self.K -= np.einsum(\"ki,j->ij\", del_K, dK) * alpha\n",
    "        \n",
    "        del_Q = np.nan_to_num(np.matmul(dL, dS))\n",
    "        self.Q -= np.einsum(\"ki,j->ij\", del_Q, dQ) * alpha\n",
    "\n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "head = AttentionHead(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "t = t[:,0:8]\n",
    "\n",
    "output = head(x)\n",
    "print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "#print(A.layers[0].weights)\n",
    "for _ in range(1000):\n",
    "    head.step(x, t[:,0:8])\n",
    "    output = head(x)\n",
    "    outputs.append(nn.cross_entropy(output, t))\n",
    "    \n",
    "plt.plot(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1218fe160>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvWElEQVR4nO3dd3xc1Znw8d8zMxqNepcsy5Y7GBcCtmKbUJIQiiEEUjZZiENZkvWygRey2RLy7oZ3X0J2l92ELMmSEEIICYE4pIHfYOJAiGmJi4wdV2zLcpMtW5Jlq1plNM/7x9yRR7LKqI5m5vl+PvPx3HPPvTpXV55nTrnniKpijDEmMbmiXQBjjDHRY0HAGGMSmAUBY4xJYBYEjDEmgVkQMMaYBGZBwBhjElhEQUBElovIHhGpEJH7+9h/h4jUishW5/W5sH23i8g+53V7WPpiEdnunPNbIiKjc0nGGGMiJYM9JyAibmAvcDVQBWwCblHVXWF57gDKVPWeXsfmAuVAGaDAZmCxqp4SkY3AvcAGYA3wLVV9eZSuyxhjTAQiqQksASpUtVJVO4BVwE0Rnv9a4BVVrVfVU8ArwHIRKQYyVXW9BqPQj4GPDr34xhhjRsITQZ4S4EjYdhWwtI98nxCRKwjWGv5OVY/0c2yJ86rqI31A+fn5On369AiKbIwxJmTz5s11qlrQ175IgkAk/h/wU1VtF5G/AX4EXDkaJxaRlcBKgNLSUsrLy0fjtMYYkzBE5FB/+yJpDjoKTA3bnuKkdVPVk6ra7mw+CSwe5Nijzvt+zxl27idUtUxVywoK+gxkxhhjhimSILAJmCMiM0TEC9wMrA7P4LTxh9wI7HberwWuEZEcEckBrgHWqmo10Cgiy5xRQbcBL47wWowxxgzRoM1BquoXkXsIfqC7gadUdaeIPAiUq+pq4F4RuRHwA/XAHc6x9SLyVYKBBOBBVa133n8eeBpIAV52XsYYY8bRoENEJ5KysjK1PgFjjBkaEdmsqmV97bMnho0xJoFZEDDGmARmQcAYYxKYBYEEcfhkK7/cXEVbZ1e0i2KMmUAsCCSIp94+wN///M+8/7/+wDN/Ooi/KxDtIhljJgALAgmiKxAcBVaam8pXXtzJx77zRw6dbIlyqYwx0WZBIEEEVMlP9/L831zCd1YsYu+JJp54ozLaxTLGRJkFgQQRUBARRITrFxaTn55Mu9+ahIxJdBYEEoS/K4DHdXbdHpFg7cAYk9hGaxZRM0wvbavmS7/cRm6al5w0LzmpSeSmBt/npnnJSfVSkJFMcZaPSVk+clO9uFxDX4StpcNPWvLZ2+0SwWKAMcaCQJS9e7yR5nY/V11QSH1rJyebO6ioaeZUSwctHecO50xyC0WZPoqzfN3/TspK6Q4SxVk+CtKT8bjPVvJUlX0nmpmU6etOc1lNwBiDBYGoC6jicQn/ffPF5+xr6+ziVGsHtU3tVDe0cbyhjeONwX+rG86w42gDr+4+QVtnz7Z9l0BhRjAo5KV5qTp1hn01zXz2shlheYSAxQBjEp4FgSgLaPADuS++JDfFWSkUZ6Vw4ZQ+s6CqNJzp7A4SwX/PUO28P9bQRn6Gl9vet4C/fO/ZpR1EgscaYxKbBYEoC6jSTwyIiIiQneolO9XLBcWZQzrOYoAxxkYHjbGaxjbO9NG2H6ID1ATGkvUJGGPAagKjrqaxjVd2n+DNvXVsPXKa441tXL9wEt9ZsbjP/PtrmjkThfl8gn0CFgSMSXQWBEZJZW0z33hlLy9vryagMCUnhaUzc3lx6zHerjjJ3hNNeN0uGs50crKlnb0nmvnT/pO8vrc2KuUV6xg2xmBBYFT8dsdxvvCzLSS5XKy8YhYfX1TCnMJ0RIT3n1fAF5//M9d8841zjivNTY1CaYNc1jFsjMGCwIjtPdHEvT/dwrzJmXzv1sUUhY3FB/j4oinMn5zFnhNN+LsCZPqSyEnzMqsgjexUL9Pvfykq5bYhosYYiDAIiMhy4FGCC80/qar/0U++TwC/AN6rquUisgL4x7AsFwKLVHWriKwDioEzzr5rVLVmeJcRPY/+fh/JSS5+cHsZeenJfeY5f1IG50/K6HPfnZfO4PW943/Z1jFsjIEIgoCIuIHHgKuBKmCTiKxW1V298mUA9wEbQmmq+izwrLN/IfCCqm4NO2yFqsbsyvGdXQFe213DxxeV9BsABvPAR+YB80a3YBGwPgFjDEQ2RHQJUKGqlaraAawCbuoj31eBh4G2fs5zi3Ns3DhS38qZzi4WleZEuyhDZn0CxhiILAiUAEfCtquctG4isgiYqqoDNXD/JfDTXmk/FJGtIvIVkSgMlh+hw/WtAEzLi14H73DZEFFjDIzCw2Ii4gIeAf5+gDxLgVZV3RGWvEJVFwKXO69b+zl2pYiUi0h5bW10hlP2J7Reb/jsnLHCJULAlhMwJuFFEgSOAlPDtqc4aSEZwAJgnYgcBJYBq0WkLCzPzfSqBajqUeffJuA5gs1O51DVJ1S1TFXLCgoKIijuubZVnaa64czgGYcotEyvexhTO0eddQwbY4hsdNAmYI6IzCD44X8z8OnQTlVtAPJD286on38Idfg6NYVPEfy2H8rjAbJVtU5EkoAbgFdHfDX9+MLPtlJZ28LkLB8XT8vh2vmTuGZeEb4k94jO63e+SsdiEHAJVhMwxgweBFTVLyL3AGsJDhF9SlV3isiDQLmqrh7kFFcAR1Q1fEHbZGCtEwDcBAPA94d1BRH45qcuYvOhU7xz+BQbD9Tz0rZqijKT+e+/vJhLZuUN+7yhb9Lu2OvOwONy0eL3R7sYxpgoi6gxW1XXAGt6pT3QT94P9NpeR7CJKDytBeh7Mp0x8J6p2bxnajZ3MoOugPLH/XX8n9U7Wfnjct760pVkpSYN67z+LicIxGBNICs1iWOnR7+JzBgTWxJuFlG3S7h8TgH/9RcX0tTu5+39dcM+V3dNIAaDQF6al5MtHdEuhjEmymJvWMsoOX9ScO790DDP3jZUnuQLP9tKXrqX6XlpTMtLpTQ3lWl5acybnEmmLwl/IJaDQDINZzrp8AfwehLuu4AxxpGwQSA92UOq101tU3uf+7cfbaC6oY3CTB/bjzbw2x3Huz/0RWDupEw6u2K3Yzj0bMOBupZ+p7QwxsS/hA0CAIUZyWw8UM8P3jqAxyW4XEIgoNQ1t/Pi1mPkpnn59d++D5dL8HcFqG5oo7KuhS2HT7FuTy27qxuB2OwYnlsc/ODfXd1oQcCYBJbQQWBBSRa/2VbN9qMNPdJdAucVZfDVjy7A5XzL97hdTM1NZWpuKu8/r4D7PjSHGV8O9pW73bEXBGYVpJPmdbPxYD0fvbhk8AOMMXEpoYPAt2+5mP/4xIV0BZSugOIPBHCJkJPqHbSJR0ScxdrBE4PNQUluF5fPKeC13TXoR5UYnLXDGDMKEjoIiAjpI5jy4Rd3XcK2qgZSvbH5a7x6XhG/3XmcjQfqWTpz+M9LGGNilw0LGYHF03L5q0tnRLsYw3b9wmIyfB6e23g42kUxxkSJBYEEluJ18/GLS3h5+/F+R0kZY+KbBYEEd9v7ptPRFeDZDYeiXRRjTBRYEEhwswrSuXJuIT9Zf6h7amxjTOKwIGC4433TqWvu4PW9E2u9BmPM2LMgYLqfHm5qs1lFjUk0FgQMLucZgYCtPG9MwrEgYLofjLOVxoxJPBYETHcQ6LIgYEzCsSBgrDnImARmQcCcrQlYEDAm4VgQMN1TYXdZDDAm4UQUBERkuYjsEZEKEbl/gHyfEBEVkTJne7qInBGRrc7r8bC8i0Vku3POb4lNYxk1LuevwJqDjEk8g05/KSJu4DHgaqAK2CQiq1V1V698GcB9wIZep9ivqhf1cervAn/t5F8DLAdeHuoFmJGzjmFjElckNYElQIWqVqpqB7AKuKmPfF8FHgbaBjuhiBQDmaq6XlUV+DHw0YhLbUZVqGPY+gSMSTyRBIES4EjYdpWT1k1EFgFTVfWlPo6fISJbROR1Ebk87JxVA53TjJ/u5wQsCBiTcEa8GoqIuIBHgDv62F0NlKrqSRFZDLwgIvOHeP6VwEqA0tLSEZbW9OVsx7AFAWMSTSQ1gaPA1LDtKU5aSAawAFgnIgeBZcBqESlT1XZVPQmgqpuB/cB5zvFTBjhnN1V9QlXLVLWsoKAgsqsyQyISfFlzkDGJJ5IgsAmYIyIzRMQL3AysDu1U1QZVzVfV6ao6HVgP3Kiq5SJS4HQsIyIzgTlApapWA40isswZFXQb8OLoXpqJlIiQl+alrrkj2kUxxoyzQZuDVNUvIvcAawE38JSq7hSRB4FyVV09wOFXAA+KSCcQAO5S1Xpn3+eBp4EUgqOCbGRQFBVl+jjecCbaxTDGjLOI+gRUdQ3BYZzhaQ/0k/cDYe9/Cfyyn3zlBJuRzARQnJXCoZMt0S6GMWac2RPDBoDzitI5UNdCu99WFzMmkVgQMABcUJyJP6DsO9Ec7aIYY8aRBQEDBIMAwO7qxiiXxBgzniwIGABm5KfhS3Lx7vGmaBfFGDOOLAgYIPjU8PlFGew6ZjUBYxKJBQHT7YLiTHYfb0TtyWFjEoYFAdPtguJMTrd2crxx0DkAjTFxwoKA6TZvsnUOG5NoLAiYbjPz0wA4dLI1yiUxxowXCwKmW26al2SPi2OnbfoIYxKFBQHTTUQozvJxrMH6BIxJFBYETA+FmT5qm9qjXQxjzDixIGB6yPR5aG7zR7sYxphxYkHA9JCe7KGpvTPaxTDGjBMLAqaHDF8STVYTMCZhWBAwPfiSXLR3BqJdDGPMOLEgYHrwuF221rAxCcSCgOnB4xI6A9GtCbR1dnH4ZKvNYWTMOIhoeUmTODwuF6oQCCgul0SlDI/+fh/fXbef6XmpXD2viGvmT2JRaQ7uKJXHmHgWUU1ARJaLyB4RqRCR+wfI9wkRUREpc7avFpHNIrLd+ffKsLzrnHNudV6FI78cM1Ied/CDNpq1gdOtwdFJ0/LSePqPB/nk439iydde5Ssv7GBb1WmrIRgzigatCYiIG3gMuBqoAjaJyGpV3dUrXwZwH7AhLLkO+IiqHhORBcBaoCRs/wpnwXkzQYS+bUe5RYj89GR+dOcSmto6eX1vLb/dcZzny4/wzPpDnF+UwYplpXyqbCq+JHd0C2pMjIukJrAEqFDVSlXtAFYBN/WR76vAw0D3nAOqukVVjzmbO4EUEUkeYZnNGAq1uASi+m1bEaccGb4kbrhwMv/z6UVs/OereOijC/AluXjgxZ1c9vBrfHfdfto6u6JYVmNiWyRBoAQ4ErZdRc9v84jIImCqqr40wHk+AbyjquFzEvzQaQr6iohYg+8E4HJuQ3SDAPT1x5CVksRnlk3jhbsv5WcrlzF/chYP//Zdrv3vN3hzX+24l9GYeDDi0UEi4gIeAf5+gDzzCdYS/iYseYWqLgQud1639nPsShEpF5Hy2lr7jz7WpDsIRK8Mg8UfEWHpzDx+dOcSnv3cUtwu4dYfbOSbr+wlYMNbjRmSSILAUWBq2PYUJy0kA1gArBORg8AyYHVY5/AU4NfAbaq6P3SQqh51/m0CniPY7HQOVX1CVctUtaygoCDS6zLDFGoOinbna6T1wktn57Pm3sv5+KISHv39Pr73RuXYFsyYOBNJENgEzBGRGSLiBW4GVod2qmqDquar6nRVnQ6sB25U1XIRyQZeAu5X1bdDx4iIR0TynfdJwA3AjtG6KDN8oeagaD4wNtT440ty841PvofCjGRbFc2YIRo0CKiqH7iH4Mie3cDzqrpTRB4UkRsHOfweYDbwQK+hoMnAWhHZBmwlWLP4/giuw4yS0LMBUW0OQpE+ewX6JyKk+zx02fBRY4YkoofFVHUNsKZX2gP95P1A2PuHgIf6Oe3iyIpoxlOsNQeFc4tYn4AxQ2TTRpgeXDHQMdwft0ts3iNjhsiCgOlhIjwnoPQ9RHQwbpdEfWirMbHGgoDpQSbKcwLDaA+ymoAxQ2dBwPQQag6KZgwY7s92ieC3IGDMkFgQMD2EmoOi+Y3aHwh0T2Q3FNYcZMzQWRAwPXRPIBfFD1N/lw5r2mi3WHOQMUNlQcD0kOQO/km0+6M3jWhnVwCve+h/mm6XRH32U2NijQUB00NhRnCS1xONbYPkHDstHX7Skoe+3pHbJfawmDFDZEHA9JCb5gXOLuwSDU1tfjJ8Qw8CLhsdZMyQWRAwPYQWaWn3R2+O/pPNHeSkeod8XGqSm5Z2/xiUyJj4ZUHA9JDsCf5JtHVGp3G93d/FsYYzlOamDvnYSVk+qhui14xlTCyyIGB68DpBoLMrOkFg17FGVGFOUfqQj52Sk0Jzu5/6lo4xKJkx8cmCgOkhNDqosys6beu/23UCEbhkZt6Qj11YkgXAlsOnRrtYxsQtCwKmB48zPt8fhZpA1alWfvKnQ1w7bxJ56UNfivo9U7NJcgsbDtSPQemMiU9DH4Jh4lroIa3OrgC/3lLF19fupeFMJ4WZyZTmpjIjP42Z+WlMz09jRn4ak7NSutcgGIl3Dp/iC6u2osD/vv6CYZ3Dl+RmUWkOb+2rG3F5jEkUFgRMDyJCkltoONPJl365nTmF6Vw9r4iapjYOnWxl44F6WjvOjhxK9riYlhcMDjPy05mRn8q0vDRy07xk+pLISknCl+TqMSFca4efk80dHK5vZfvRBl7ddYLyQ6coykzmmc8uoTRv6J3CIZfPyefrv9tLXXM7+cOoTRiTaCwImHN43S42HjxFhz/A3111HlfNK+rep6rUNrVTWdfCgbDX/toWXnu3ps++BI9LwtYp0HMmeTuvKJ1/Wn4+t10ynfRhPCQW7vI5BXz9d3t5u6KOmy4qGdG5jEkEFgTMOT44t5DfbKsGIDXZ3WOfiFCY6aMw08eyXp23/q4Ax063cbi+ldNnOmg400njGT9NbZ2Ef+xnpSSRm+alOMvHgslZ5KQN/ZmA/iwoySIrJYk391kQMCYSFgTMOe790JzuIBB6biASHreL0rzUETXnjJTbJVw6O4+39tWhqsNal8CYRBLR/3ARWS4ie0SkQkTuHyDfJ0RERaQsLO3LznF7ROTaoZ7TjL85hWfH6Hvd7gFyTkyXzyngeGMbFTXN0S6KMRPeoEFARNzAY8B1wDzgFhGZ10e+DOA+YENY2jzgZmA+sBz4joi4Iz2niY7wb8/eIdQEJorLZucDsL7yZJRLYszEF8n/8CVAhapWqmoHsAq4qY98XwUeBsKf278JWKWq7ap6AKhwzhfpOU2UTMlJASDVG3s1gSk5KXg9LqpOnYl2UYyZ8CIJAiXAkbDtKietm4gsAqaq6ksRHjvoOU10vXD3pTx680XdwSCWiAjFWT6O2TxCxgxqxB3DIuICHgHuGHFp+j7/SmAlQGlp6Vj8CNOH/PTkmB5dU5iRTE0U10QwJlZEUhM4CkwN257ipIVkAAuAdSJyEFgGrHY6h/s7drBzdlPVJ1S1TFXLCgoKIiiuMZCe7OnxUJsxpm+RBIFNwBwRmSEiXoIdvatDO1W1QVXzVXW6qk4H1gM3qmq5k+9mEUkWkRnAHGDjYOc0ZqTSkj22toAxERi0OUhV/SJyD7AWcANPqepOEXkQKFfVfj+8nXzPA7sAP3C3qnYB9HXOkV+OMUGpXjctHRYEjBlMRH0CqroGWNMr7YF+8n6g1/bXgK9Fck5jRkuS24U/StNhGxNLYm8QuDERSHK7orYwjjGxxIKAiUsel5wzUZ0x5lwWBExccruELgsCxgzKgoCJSy6XoBYDjBmUBQETl1wCXRYFjBmUBQETl1wiBCwIGDMoCwImLrkk2BykFgiMGZAFAROXzi5nGeWCGDPBWRAwccnt/GVbk5AxA7MgYOJSaGGc0RwmqqocPX2G8oP1BKyKYeKErTFs4pLbFQwCo1ERqKhp4odvH+SVXSeoaWoH4NnPLeVSZwUzY2KZBQETl5wYMOJhok++Wcm/rdlNssfNB+cWUJqbxuOv76fxTOcolNKY6LMgYOLS2Y7h4QeBN/bW8tBLu1k+fxJf+9gC8tKTqahp5vHX99Nh8xKZOGFBwMSl7iAwgrb7779ZSUl2Co/echHJnuBay16nx7nTZig1ccI6hk1cCvUJDDcGBALKO4dOcfW8ou4AAOD1hIKA1QRMfLAgYOJSd5/AMKNAXXM7LR1dzCpI65Ge5A6e2IKAiRcWBExccnWPDhpeEGj3Bz/kfUnuHulJTk2gw29BwMQHCwImLoX6BIY7OihUg/A43/xDrE/AxBsLAiYuhZqDhtsnEFqQxu3q+V8kyW19Aia+RBQERGS5iOwRkQoRub+P/XeJyHYR2Soib4nIPCd9hZMWegVE5CJn3zrnnKF9haN6ZSahjXR0kD8Q/JD3uHrWBNwuIdnj4nSrPSdg4sOgQUBE3MBjwHXAPOCW0Id8mOdUdaGqXgT8J/AIgKo+q6oXOem3AgdUdWvYcStC+1W1ZsRXY4wjNDroxa1HaevsGvLxoUXq3b2CAMD8yZmUH6ofWQGNmSAiqQksASpUtVJVO4BVwE3hGVS1MWwzDejr69ctzrHGjLnLZuezbGYuX//dXpZ87VW+8sIONlSejLgZp7tPoI8gcMOFk9lW1cCv3qka1TIbEw2RPCxWAhwJ264ClvbOJCJ3A18EvMCVfZznL+kVPIAfikgX8EvgIbXJ380oKcz0sWrlJWyoPMlPNx7m+fIjPLP+EBk+D5fNzue903O5uDSb+ZOzusf+hzvbJ3BuEFixrJS1O4/zxef/zCu7TnDLklKWzMg9ZySRMbFg1J4YVtXHgMdE5NPAvwC3h/aJyFKgVVV3hB2yQlWPikgGwSBwK/Dj3ucVkZXASoDS0tLRKq5JEEtn5rF0Zh5fbevk7Yo61u2p5c19dby84zgQfPjrgkkZnFfkvCZlcF5ROn6nxhDqCA6X7HHz488u4bHXKnj6jwd5ecdxfEku5k/OYq5zrtLcVJbOzCXVaw/lm4lNBvvyLSKXAP+qqtc6218GUNV/7ye/Czilqllhad8EalX13/o55g6gTFXvGagsZWVlWl5ePmB5jYnE8YY2thw+xTuHT7GrupE9x5upa27v3u+S4MiiVSuXsWxmXr/nOdPRxZ8q63hzXx07jzXybnUjjW1+AO5433T+9cb5Y34txgxGRDarallf+yL5mrIJmCMiM4CjwM3Ap3v9gDmqus/Z/DCwL2yfC/gUcHlYmgfIVtU6EUkCbgBejfySjBmZSVk+rltYzHULi7vTTrV0sPdEE3tPNFF1+gxtHV1cOCVrgLNAitfNlXOLuHJuERB8OK22qZ2PfeeP1Ld0jOk1GDMaBg0CquoXkXuAtYAbeEpVd4rIg0C5qq4G7hGRq4BO4BRhTUHAFcARVa0MS0sG1joBwE0wAHx/VK7ImGHKSfN2Nx8Nl4hQmOkjw+cZ1qgkY8ZbRA2WqroGWNMr7YGw9/cNcOw6YFmvtBZg8VAKakwsSU5y02ZTS5gYYE8MGzMGfB4Xre3+aBfDmEFZEDBmDJw/KYMdxxqobWofPLMxUWRBwJgxcNsl0wgofPLxP/Ly9mqba8hMWBYEjBkDswszePZzSxER/vbZd7js4ddobLP5hszEY0HAmDHy3um5vPJ3V3DvlbM50dhuTUNmQrIgYMwY8rhdzC7KAIa/wI0xY8mCgDFjzB1a4Ma6BcwEZEHAmDEWmn5ouOsdGzOWLAgYM8YktMBNlJuD7Alm0xeb4tCYMeaeAEHgmfWH+MoLO5hVkMbiaTlcXJrDgslZnDcpnWSPTYGdyCwIGDPGQmsSRLM5qPr0GQCm56Xxyq4TPF8eXBDH4xJmF6azoCSL+ZMzmT85i3mTM0lPto+GRGF32pgxJiNc9H40dKni9bj4wR3vRVU5XN/KzmON7DzWwM5jjazbU8svNp9dKW1GfhrzijM5f1IGcydlcEFxJiXZKbj6WGTHxDYLAsaMsVBNIJrNQYGAdjdLiQjT8tKYlpfG9WFTadc0trHzWCM7jjZ0B4g1O6oJFTs92cN5RenMLc7kgkkZzHWCRKYvKRqXZEaJBQFjxtjZIaLRCwKtHV2keAdu+y/M9FGY6eODcwu701ra/ew90cS7x5vYc7yJ3dWNvLStmuc2HO7OU5KdwtxJGRRn+/C4XLxnahYXT81hWl5qd6d4pPxdATYeqKe2uZ15xZnMcZ6xMGPHgoAxYyzUhBIY4yCgqjS3+6lr7qCuOfiEck1jGwdPtlJ+8BQl2SlDPmdasoeLS4MdyeE/53hjG+9WB4PDu8cbebe6ibf31+HvUp7+Y/A6c1KTuGhqtnN8NhdOySYrpf9ag6py54/KeWNvLQBzCtN55YvvH3KZzdBYEDBmjLm6RweN/rl3Hmvge69XsuNoA9UNbZwZYBjoP157/qj8TBGhOCuF4qyUHrUGCNZ29p5oYuuR02w5fIoth0+zbm9td5PS7MJ0FpfmsGxWLpfMzGdSlq/72MP1rbyxt5aVV8xke1UDe080jUp5zcAsCBgzxrofFhvlPoHGtk5ueWI9IsL7ZuXxwbmFFGYkU5CRTH6688rw8vPyKjwu4XOXzxzVn98Xt0u4oDiTC4ozuWVJaXc5tx1pCAaFI6d5eUc1Pys/AgQ7oJfNzGXZzDxSkoLNVctm5iLAO4dPjXl5jQUBY8Zcd01glKsCB+taaGzz8/hnFrF8QXG/+e7+4OxR/blDlelL4rI5+Vw2Jx8I1hZ2VzeyvvIk6ytP8ptt1fx045Hu/Bm+JNKTPbT7A3T4A3g99kzrWLIgYMwYc43iw2LPbzrCvpomCjN8nGoNLmSfk+od8XnHk9slLCjJYkFJFp+7fCZdAWXXsUbeqqjjYF0LCyZnsb+mGYDD9S3MLrTO4bFkQcCYMTaaD4s99NIuGtt6LltZlOnrJ3dscLuEhVOyWDglqzvt/ecX4HEJ336tgm9+6iJ7PmEMRRQERGQ58CjgBp5U1f/otf8u4G6gC2gGVqrqLhGZDuwG9jhZ16vqXc4xi4GngRSCi9jfpzbXrolDo1kTcLuEW5dN4x+uPZ/apjYCCtPz00Z83ommOCuFez80h0de2UtnV4B/+9hCsmOsxhMrBg0CIuIGHgOuBqqATSKyWlV3hWV7TlUfd/LfCDwCLHf27VfVi/o49XeBvwY2EAwCy4GXh3kdxkxYoTbtdv/I55JWwCWQlZI04HDLePC/rpyN1+Pi62v3sOngKb560/wB+z7M8ETS47IEqFDVSlXtAFYBN4VnUNXGsM00gn+r/RKRYiBTVdc73/5/DHx0KAU3JlYUZSYDcKKxbcTnUmXID2DFKhHhrvfP4oW7LyU/PZm7fvIOdz2zmZpR+D2asyIJAiXAkbDtKietBxG5W0T2A/8J3Bu2a4aIbBGR10Xk8rBzVoXl6fOcznlXiki5iJTX1tZGUFxjJpbQaJdjp0cjCCRei+mCkixW33Mp/7T8fF7bU8OHHnmdVRsPJ+TvYiyM2tgrVX1MVWcBXwL+xUmuBkpV9WLgi8BzIpI5xPM+oaplqlpWUFAwWsU1ZlzNnZTBn6tOj/g8qmf7GBJJktvF5z8wm7VfuIJ5xZnc/6vt3PL99Ryoa4l20WJeJEHgKDA1bHuKk9afVThNO6rarqonnfebgf3Aec7xU4ZwTmNi2rKZeWyrahjxYvPtXQGSPIkXBEJm5Kfx079exr9/fCE7jzVy3aNv8Na+umgXK6ZFEgQ2AXNEZIaIeIGbgdXhGURkTtjmh4F9TnqB07GMiMwE5gCVqloNNIrIMgk2cN4GvDjiqzFmgvrYohK6AsqP/3Rw2OfoCigd/kDCLwLjcgm3LCnl1S++n+l5aXz2R5vYXtUQ7WLFrEGDgKr6gXuAtQSHez6vqjtF5EFnJBDAPSKyU0S2Emz2ud1JvwLY5qT/ArhLVeudfZ8HngQqCNYQbGSQiVuzCtK54cJivvd6JbuONQ5+QB9ONgdrEQXpNlQSgs9HPPu5peSmebl31RY6RmH0VSKSWOpcKSsr0/Ly8mgXw5hhqW1q5yPffgsReOOfPkiSe2hdcuv21HDHDzfxk88u7Z6CwcAf3q3hr57exH98fCE3O/MVmZ5EZLOqlvW1zyblMGacFGQks/KKmVQ3tFHf0jGkYwMB5ek/HiTD52HRtOyxKWCM+sD5BZxXlM4LW61bcTgsCBgzjjKdB7zaOyNvumhs6+R/rdrCuj21fOGq80j12mwv4USEK+cWsengKdoGmErb9M3+mowZR76k0NPDkX1YbTpYzxdWbeV4Yxv/eO353Hnp9DEsXeyaNzmTroBy8GQLcycNaRR6wrMgYMw4Co3saYugJvDnI6f5zJMbmJTl4+d3XcKisNW9TE8l2cFJ9E40tjN3UpQLE2MsCBgzjpKdeYTaIqgJ/NfaPWSlJPHrz19KbpqNCBpIaLH7hjOdUS5J7LE+AWPGUWja5yP1rQPm6/AH2HiwnhvfM9kCQATSfcHvs829ptk2g7MgYMw4ml2YTqbPwx/2DDwP1unWDjr8gbicJnosdK/ZEEND3icKCwLGjCO3S/hk2VRe3l7N7ur+HxrrdBag8Q7xWYJE5XEFf0+jvYRnIrC/MGPG2d0fnE12ahJ3P/dO91PAvXU6T7963Ik7T9BQuJ1J9fwWBIbMgoAx4yw3zct3Vizm6KkzfPJ7f6KipumcPP5AMAgM9aniROV2h5bwtKkjhsr+woyJgiUzcvnxnUtoaO3kw996i0df3Udrx9lOzQ5/8BttktUEIuLpXsc5ygWJQRYEjImSpTPzWHPf5Vw1r4hvvrqXS/79Nf795d3sONpAZ5fVBIaiu2PYagJDZs8JGBNFRZk+Hvv0Iu689BRPvlnJ99+o5HuvV5LhDHn0WBCIiPUJDJ8FAWMmgMXTclg8bTH1LR28uvsEb1fUUX26jQsmZUS7aDHB5RJEgmsumKGxIGDMBJKb5uVTZVP5VNnUwTObHjwusSAwDFbXNMbEBbcFgWGxIGCMiQsel8v6BIbBmoOMMXHBl+TmV+9U4Utyceuy6UzK8kW7SDHBagLGmLjw3c8sYvG0XL67bj8f+Pof+Pbv91nzUAQiCgIislxE9ohIhYjc38f+u0Rku4hsFZG3RGSek361iGx29m0WkSvDjlnnnHOr8yocvcsyxiSa907P5cnby1j3Dx/kyrmFfOOVvdz1k80WCAYxaBAQETfwGHAdMA+4JfQhH+Y5VV2oqhcB/wk84qTXAR9R1YXA7cAzvY5boaoXOa+aEVyHMcYAUJqXyndWLOYrN8zjlV0nePLNymgXaUKLpCawBKhQ1UpV7QBWATeFZ1DV8OkQ0wB10reo6jEnfSeQIiLJIy+2McYM7LOXzeDyOfk89fYBdAhTTN/59CbKHnqFB17cwet7a+O+JhFJECgBjoRtVzlpPYjI3SKyn2BN4N4+zvMJ4B1VDZ828YdOU9BXRKTPSVJEZKWIlItIeW3twHOwG2NMuA8vLOZEYzsHTw68iE+4TQfrqWvu4OflVdz+1Eau/MY6Xt5ePYaljK5R6xhW1cdUdRbwJeBfwveJyHzgYeBvwpJXOM1ElzuvW/s57xOqWqaqZQUFBaNVXGNMApjhLMpz7PSZiI/xJbm5ZclUtjxwNd9ZsYg0r4e/ffYdXtx6dKyKGVWRBIGjQPjji1OctP6sAj4a2hCRKcCvgdtUdX8oXVWPOv82Ac8RbHYyxphRk5ceXJqzrp91G/rSFVDcLsGX5Ob6hcW8cPelzJ+cyeOvx2ffQiRBYBMwR0RmiIgXuBlYHZ5BROaEbX4Y2OekZwMvAfer6tth+T0iku+8TwJuAHaM4DqMMeYcGc4C9E1DWHu4K6DdE9IBeD0u3jcrj8ra5iH1LcSKQR8WU1W/iNwDrAXcwFOqulNEHgTKVXU1cI+IXAV0AqcIjgQCuAeYDTwgIg84adcALcBaJwC4gVeB74/idRljDOnJwY+4lvbIgkBXQGlu95OW3POjsSjTR7s/wDd+t5fp+WlMzUlham4qRZm+7mmsY1VETwyr6hpgTa+0B8Le39fPcQ8BD/Vz2sURltEYY4Yl1etGJPIgsG5PDV0B5byinrO3LpuZR0l2Co+tqyC8MpDkFkqyU5iSk8rU3NC/qd1BIi/NSz9jXiYMmzbCGBO3RIRMXxL/84cKnll/iOxUL1kpSeSkJpHuS8IlIEC7P8Dh+lZ2HmtkZn4a186f1OM8C0qyePv+K2n3d3HsdBtH6ls5cqqVI/VnOHKqlar6Vn63s5GTLR09jktJcjMlJ4VpeWnMKkxjVkE6swvTmVWQTlZK0jj+JvonsdTGVVZWpuXl5dEuhjEmhrz27gm2HD7N6dZOTp/p5HRrB6dbO2lq60QB1eA3+snZKSydkctnlk0jO9U7rJ/V0u6n6tSZ7iARen/wZAsH61rpCFv/siAjmVkFaZxflMF7pmZz4ZRsZuan4RqD5iUR2ayqZX3usyBgjDFjz98VoOrUGSpqmtlf20xFTTMVtc3sOd5Ea0cXABnJHi4qzeb95xVw5dxCZhakj8rPtiBgjDETVFdAqahp5s9Vp/nzkdNsOFBPRU0zAAtKMrn9kul87OKSES01akHAGGNiyJH6Vl7dfYLnNhxmX00zcydl8KM7l1CUObzpsQcKAtYxbIwxE8zU3FT+6tIZ3PG+6fx2x3F+veUo+eljM+2aBQFjjJmgRITrFhZz3cLiMfsZtqiMMcYkMAsCxhiTwCwIGGNMArMgYIwxCcyCgDHGJDALAsYYk8AsCBhjTAKzIGCMMQkspqaNEJFa4NAwD88H6kaxOLHArjkxJNo1J9r1wsiveZqq9rlIe0wFgZEQkfL+5s6IV3bNiSHRrjnRrhfG9pqtOcgYYxKYBQFjjElgiRQEnoh2AaLArjkxJNo1J9r1whhec8L0CRhjjDlXItUEjDHG9BL3QUBElovIHhGpEJH7o12ekRCRqSLyBxHZJSI7ReQ+Jz1XRF4RkX3OvzlOuojIt5xr3yYii8LOdbuTf5+I3B6ta4qUiLhFZIuI/MbZniEiG5xr+5mIeJ30ZGe7wtk/PewcX3bS94jItVG6lIiISLaI/EJE3hWR3SJySbzfZxH5O+fveoeI/FREfPF2n0XkKRGpEZEdYWmjdl9FZLGIbHeO+ZaIDL5qvarG7QtwA/uBmYAX+DMwL9rlGsH1FAOLnPcZwF5gHvCfwP1O+v3Aw87764GXAQGWARuc9Fyg0vk3x3mfE+3rG+Tavwg8B/zG2X4euNl5/zjwt877zwOPO+9vBn7mvJ/n3P9kYIbzd+GO9nUNcL0/Aj7nvPcC2fF8n4ES4ACQEnZ/74i3+wxcASwCdoSljdp9BTY6ecU59rpByxTtX8oY/8IvAdaGbX8Z+HK0yzWK1/cicDWwByh20oqBPc777wG3hOXf4+y/BfheWHqPfBPtBUwBfg9cCfzG+QOvAzy97zOwFrjEee9x8knvex+eb6K9gCznA1F6pcftfXaCwBHng83j3Odr4/E+A9N7BYFRua/OvnfD0nvk6+8V781BoT+skConLeY51d+LgQ1AkapWO7uOA0XO+/6uP9Z+L/8N/BMQcLbzgNOq6ne2w8vffW3O/gYnfyxd8wygFvih0wT2pIikEcf3WVWPAl8HDgPVBO/bZuL7PoeM1n0tcd73Th9QvAeBuCQi6cAvgS+oamP4Pg1+BYibIV8icgNQo6qbo12WceQh2GTwXVW9GGgh2EzQLQ7vcw5wE8EAOBlIA5ZHtVBREI37Gu9B4CgwNWx7ipMWs0QkiWAAeFZVf+UknxCRYmd/MVDjpPd3/bH0e7kUuFFEDgKrCDYJPQpki4jHyRNe/u5rc/ZnASeJrWuuAqpUdYOz/QuCQSGe7/NVwAFVrVXVTuBXBO99PN/nkNG6r0ed973TBxTvQWATMMcZYeAl2IG0OsplGjanp/8HwG5VfSRs12ogNELgdoJ9BaH025xRBsuABqfauRa4RkRynG9g1zhpE46qfllVp6jqdIL37zVVXQH8AfgLJ1vvaw79Lv7Cya9O+s3OqJIZwByCnWgTjqoeB46IyPlO0oeAXcTxfSbYDLRMRFKdv/PQNcftfQ4zKvfV2dcoIsuc3+FtYefqX7Q7ScahE+Z6gqNo9gP/HO3yjPBaLiNYVdwGbHVe1xNsC/09sA94Fch18gvwmHPt24GysHPdCVQ4r7+K9rVFeP0f4OzooJkE/3NXAD8Hkp10n7Nd4eyfGXb8Pzu/iz1EMGoiytd6EVDu3OsXCI4Ciev7DPxf4F1gB/AMwRE+cXWfgZ8S7PPoJFjj++xo3legzPn97Qf+h16DC/p62RPDxhiTwOK9OcgYY8wALAgYY0wCsyBgjDEJzIKAMcYkMAsCxhiTwCwIGGNMArMgYIwxCcyCgDHGJLD/DzFPzxCTmIitAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 seq_len=SEQUENCE_MAX, masked=False):\n",
    "        # Class vars\n",
    "        self.attn_dim = embed_dim // n_heads\n",
    "        \n",
    "        # Create all heads and weights for multiheaded attention\n",
    "        self.heads = [AttentionHead(embed_dim, n_heads) for _ in range(n_heads)]\n",
    "        self.O = np.random.randn(embed_dim, seq_len) * np.sqrt(2 / embed_dim)\n",
    "        \n",
    "        # If the attention block is masked\n",
    "        self.mask = None\n",
    "        if masked:\n",
    "            self.mask = np.ones((self.attn_dim, self.attn_dim)) * -np.inf\n",
    "            self.mask = np.triu(self.mask, k=1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward passes through concatenated heads and matmuls by weights\"\"\"\n",
    "        h_cat = np.concatenate([h(x, self.mask) for h in self.heads], axis=1)\n",
    "        self.layer_output = np.nan_to_num(h_cat * self.O.T)\n",
    "        return(self.layer_output)\n",
    "        \n",
    "    def backprop(self, x, target, alpha=0.0001):\n",
    "        \"\"\"Makes one step along each gradient of each attention head\"\"\"\n",
    "        for i, h in enumerate(self.heads):\n",
    "            t = target[:,0 + i * self.attn_dim:(1 + i) * self.attn_dim] #0-32, 33-64, etc.\n",
    "            h.step(x, t, alpha)\n",
    "        \n",
    "        # Derivative of layer given output from multihead attention\n",
    "        pred = self.__call__(x)\n",
    "        delta = -(target / pred) + (np.ones(target.shape) - target) / \\\n",
    "                                   (np.ones(pred.shape) - pred)\n",
    "        self.O -= np.multiply(delta, pred).T * alpha\n",
    "        \n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "attn = MultiheadAttention(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "output = attn(x)\n",
    "#print(output.shape)\n",
    "#print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "#print(A.layers[0].weights)\n",
    "for _ in range(10000):\n",
    "    attn.backprop(x, t)\n",
    "    output = attn(x)\n",
    "    loss = nn.cross_entropy(output, t)\n",
    "    outputs.append(loss)\n",
    "    \n",
    "plt.plot(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Net):\n",
    "    \"\"\"Position-wise feed forward nueral network\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        super(FFN, self).__init__()\n",
    "        self.L1 = nn.LinearLayer(embed_dim, inner_dim, sequence_max)\n",
    "        self.L2 = nn.LinearLayer(inner_dim, embed_dim, sequence_max)\n",
    "        \n",
    "        self.layers = [\n",
    "            self.L1,\n",
    "            self.L2,\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "        x = self.L2(np.maximum(self.L1(x), 0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement layer norm\n",
    "def layer_norm(layer):\n",
    "    std = np.std(layer)\n",
    "    mean = np.mean(layer)\n",
    "    print(std, mean)\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    \n",
    "    \"\"\"Encoder block for the transformer.\n",
    "    Args:\n",
    "            embed_dim (string): Directory with all the images.\n",
    "            n_heads (string): Path to the csv file with annotations.\n",
    "            sequence_max (int):\n",
    "            inner_dim (int):\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.sequence_max = sequence_max\n",
    "        \n",
    "        self.multihead_attn = MultiheadAttention(embed_dim, n_heads)\n",
    "        self.feedforward = FFN(embed_dim, inner_dim, sequence_max)\n",
    "    \n",
    "    def temp_add_norm(self, x, y):\n",
    "        # TODO replace with layer norm function\n",
    "        x = np.add(x, y) \n",
    "        return (x / np.sqrt(np.sum(x**2))).T\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = self.multihead_attn(x)\n",
    "        x = self.temp_add_norm(x, y.T)\n",
    "        \n",
    "        y = self.feedforward(x)\n",
    "        x = self.temp_add_norm(x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#multihead_attn = MultiheadAttention()\n",
    "#ffn = FFN()\n",
    "#\n",
    "#x = dataset[0][\"input_embedding\"]\n",
    "#t = dataset[0][\"input_embedding\"]\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "#\n",
    "##multihead_attn.backprop(x, t)\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "\n",
    "\n",
    "#x = ffn(t.T)\n",
    "#\n",
    "#\n",
    "#ffn.backprop(x, t)\n",
    "#print(nn.loss(x, t))\n",
    "\n",
    "#for i in range(x.shape[1]):\n",
    "#    print(x[:,i].shape, t.T[:,i].shape)\n",
    "#    ffn.backprop(x[:,i], t.T[:,i])\n",
    "#    print(nn.loss(x, t.T))\n",
    "    \n",
    "\n",
    "#layer_norm(x)\n",
    "#enc_block = EncoderBlock()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
