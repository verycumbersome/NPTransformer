{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:19.632Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import fasttext.util\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nn\n",
    "import utils\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MODEL_DIM = 64\n",
    "INNER_DIM = MODEL_DIM * 4\n",
    "\n",
    "SEQUENCE_MAX = 128 # Max length of input or output sentence\n",
    "NUM_HEADS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "embeddings = utils.get_embeddings([\"en\", \"fr\"], dim=MODEL_DIM) # using dim 256 instead of 512\n",
    "\n",
    "en_emb = embeddings[\"en\"]\n",
    "fr_emb = embeddings[\"fr\"]\n",
    "\n",
    "print(en_emb.get_dimension())\n",
    "print(fr_emb.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-31T02:42:20.695Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_embedding(x, pad_len, embed_dim):\n",
    "    \"\"\"Pads a word embedding to a max length\"\"\"\n",
    "    padded = np.zeros((pad_len - x.shape[0], embed_dim))\n",
    "    return np.concatenate((x, padded))\n",
    "\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"Dataset for the position encoded and word embedded translations\"\"\"\n",
    "    def __init__(self, inputs, targets, embeddings, pad_len, embed_dim):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        \n",
    "        # Encoders for both languages\n",
    "        emb_in = embeddings[\"en\"]\n",
    "        emb_tgt = embeddings[\"fr\"]\n",
    "        \n",
    "        # Embed all inputs\n",
    "        self.input_em = []\n",
    "        for seq in inputs:\n",
    "            embed = np.array([emb_in.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.input_em.append(np.array(embed))\n",
    "            \n",
    "        # Embed all outputs \n",
    "        self.target_em = []\n",
    "        for seq in targets:\n",
    "            embed = np.array([emb_tgt.get_word_vector(w) for w in seq.split()])\n",
    "            embed = pad_embedding(embed, pad_len, embed_dim)\n",
    "            self.target_em.append(np.array(embed))\n",
    "    \n",
    "    def pad_embedding(self, x):\n",
    "        padded = np.zeros((self.pad_length - x.shape[0], self.embed_dim))\n",
    "        return np.concatenate((x, padded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sequence))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return({\n",
    "            \"input\":self.inputs[idx],\n",
    "            \"target\":self.targets[idx],\n",
    "            \"input_embedding\":self.input_em[idx],\n",
    "            \"target_embedding\":self.target_em[idx],\n",
    "        })\n",
    "    \n",
    "data = [\"the cat likes oranges\", \"hello my friend\"]\n",
    "target = [\"le chat aime les oranges\", \"bonjour mon amie\"]\n",
    "\n",
    "dataset = TranslationDataset(data, target, embeddings, SEQUENCE_MAX, MODEL_DIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T02:42:15.861997Z",
     "start_time": "2021-01-31T02:42:15.857166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_encoding(seq):\n",
    "    \"\"\"Adds positional encoding to a sequence of word vectors\"\"\"\n",
    "    seq_len = seq.shape[0]\n",
    "    d_model = seq.shape[1]\n",
    "    \n",
    "    encoding = []\n",
    "    for i in range(seq_len):\n",
    "        w = 1 / (10000 ** ((2 * i) / d_model))\n",
    "\n",
    "        wi_s = [math.sin(p * w) * (i % 2) for p in range(d_model)]\n",
    "        wi_c = [math.cos(p * w) * ((i + 1) % 2) for p in range(d_model)]\n",
    "        \n",
    "        encoding.append(np.add(wi_s, wi_c))\n",
    "        \n",
    "    encoding = np.array(encoding)\n",
    "    \n",
    "    return np.add(encoding, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulae for gradients:\n",
    "* ### K partial derivative:\n",
    "    1 / d_k * (x * (x.T * Q)) ⊗ Identity\n",
    "\n",
    "* ### Q partial derivative:\n",
    "    1 / d_k * Identity ⊗ (k * x * x.T)\n",
    "\n",
    "* ### V partial derivative:\n",
    "    −(S⊤⋅(t⊘t0)⋅x⊤−S⊤⋅((vector(1)−t)⊘(vector(1)−t0))⋅x⊤)\n",
    "        where t0 = S * V * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 8)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-0f613be83836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class AttentionHead():\n",
    "    \"\"\"Scaled dot product attention head. \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = embed_dim // n_heads\n",
    "        self.dim_k = math.sqrt(self.embed_dim // self.output_dim)\n",
    "        \n",
    "        self.layers = [np.random.randn(self.output_dim, embed_dim) * \\\n",
    "                                 np.sqrt(2 / embed_dim) for _ in range(3)]\n",
    "        \n",
    "        #TODO find clean alternative\n",
    "        self.V, self.K, self.Q = self.layers[0], self.layers[1], self.layers[2]\n",
    "    \n",
    "    def __call__(self, x, mask=None):\n",
    "        \"\"\"Attention forward pass\n",
    "        Function: softmax(QK^T/sqrt(dim_k) * V)\n",
    "        \"\"\"\n",
    "        # Scaled dot product\n",
    "        scaled = np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k\n",
    "        \n",
    "        # Masking (Optional)\n",
    "        if mask is not None:\n",
    "            scaled = np.add(mask, scaled)\n",
    "            \n",
    "        self.layer_output = np.matmul(nn.softmax(scaled), np.dot(self.V, x.T)).T\n",
    "        return(self.layer_output)\n",
    "    \n",
    "    def F(self, x):\n",
    "        \"\"\"KQ^T/d_k for backprop\"\"\"\n",
    "        return(np.dot(np.dot(self.Q, x.T), np.dot(self.K, x.T).T) / self.dim_k)\n",
    "    \n",
    "    def step(self, x, t, alpha=0.0001, eps=1e-9):\n",
    "        \"\"\"Step along gradients to update K, Q, V weights.\"\"\"\n",
    "        one = (np.ones(self.layer_output.shape))\n",
    "        \n",
    "        # Softmax derivative\n",
    "        dS = np.matmul(nn.softmax(self.F(x)), \n",
    "                       (np.identity(self.output_dim) - nn.softmax(self.F(x))))\n",
    "        \n",
    "        # Cross entropy derivative\n",
    "        dL = np.divide(-t, self.layer_output + eps) + \\\n",
    "             np.divide((one - t), (one - self.layer_output))\n",
    "        \n",
    "        # Calculate loss w.r.t weight gradients\n",
    "        # dL/dK\n",
    "        dK = np.matmul(self.Q, x.T)\n",
    "        dK = np.matmul(dK, x).T\n",
    "        dK = np.diag(1 / self.dim_k * np.kron(dK, np.identity(self.output_dim)))\n",
    "        \n",
    "        # dL/dQ\n",
    "        dQ = np.matmul(self.K, np.matmul(x.T, x))\n",
    "        dQ = np.kron(np.identity(self.output_dim), self.K)\n",
    "        dQ = np.diag(1 / self.dim_k * dQ)\n",
    "        \n",
    "        # dL/dV\n",
    "        \n",
    "        # Calc deltas and update weights\n",
    "        del_K = np.nan_to_num(np.matmul(dL, np.nan_to_num(dS)))\n",
    "        self.K -= np.einsum(\"ki,j->ij\", del_K, dK) * alpha\n",
    "        \n",
    "        del_Q = np.nan_to_num(np.matmul(dL, dS))\n",
    "        self.Q -= np.einsum(\"ki,j->ij\", del_Q, dQ) * alpha\n",
    "\n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "head = AttentionHead(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "t = t[:,0:8]\n",
    "\n",
    "output = head(x)\n",
    "print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "#print(A.layers[0].weights)\n",
    "for _ in range(1000):\n",
    "    head.step(x, t[:,0:8])\n",
    "    output = head(x)\n",
    "    outputs.append(nn.cross_entropy(output, t))\n",
    "    \n",
    "plt.plot(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64)\n",
      "0.26574954028452874\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n",
      "(8, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ff39670>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultiheadAttention():\n",
    "    \"\"\"Multiheaded attention transformer block\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 seq_len=SEQUENCE_MAX, masked=False):\n",
    "        # Class vars\n",
    "        self.attn_dim = embed_dim // n_heads\n",
    "        \n",
    "        # Create all heads and weights for multiheaded attention\n",
    "        self.heads = [AttentionHead(embed_dim, n_heads) for _ in range(n_heads)]\n",
    "        \n",
    "        self.O = np.random.randn(embed_dim, seq_len) * np.sqrt(2 / embed_dim)\n",
    "        #self.O = nn.LinearLayer(embed_dim, embed_dim)\n",
    "        \n",
    "        # If the attention block is masked\n",
    "        self.mask = None\n",
    "        if masked:\n",
    "            self.mask = np.ones((self.attn_dim, self.attn_dim)) * -np.inf\n",
    "            self.mask = np.triu(self.mask, k=1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward passes through concatenated heads and matmuls by weights\"\"\"\n",
    "        h_cat = np.concatenate(np.array([h(x, self.mask) for h in self.heads]), axis=1)\n",
    "        return(np.multiply(self.O, h_cat.T).T)\n",
    "        \n",
    "    def backprop(self, x, t, alpha=0.001):\n",
    "        # Makes one pass across all concatonated heads of multiheaded attention\n",
    "        for i, h in enumerate(self.heads):\n",
    "            t = x.T[0 + i * self.attn_dim:(1 + i) * self.attn_dim] #0-32, 33-64, etc.\n",
    "            h.step(x, t.T, alpha)\n",
    "                \n",
    "        \n",
    "x = dataset[0][\"input_embedding\"]\n",
    "t = dataset[0][\"input_embedding\"]\n",
    "\n",
    "attn = MultiheadAttention(MODEL_DIM, NUM_HEADS)\n",
    "\n",
    "output = attn(x)\n",
    "print(output.shape)\n",
    "print(nn.cross_entropy(output, t))\n",
    "\n",
    "outputs = []\n",
    "#print(A.layers[0].weights)\n",
    "for _ in range(10):\n",
    "    attn.backprop(x, t[:,0:8])\n",
    "    output = attn(x)\n",
    "    outputs.append(nn.cross_entropy(output, t))\n",
    "    \n",
    "plt.plot(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Net):\n",
    "    \"\"\"Position-wise feed forward nueral network\"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        super(FFN, self).__init__()\n",
    "        self.L1 = nn.LinearLayer(embed_dim, inner_dim, sequence_max)\n",
    "        self.L2 = nn.LinearLayer(inner_dim, embed_dim, sequence_max)\n",
    "        \n",
    "        self.layers = [\n",
    "            self.L1,\n",
    "            self.L2,\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ReLU(xW1 + b1)W2 + b2\"\"\"\n",
    "        x = self.L2(np.maximum(self.L1(x), 0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement layer norm\n",
    "def layer_norm(layer):\n",
    "    std = np.std(layer)\n",
    "    mean = np.mean(layer)\n",
    "    print(std, mean)\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    \n",
    "    \"\"\"Encoder block for the transformer.\n",
    "    Args:\n",
    "            embed_dim (string): Directory with all the images.\n",
    "            n_heads (string): Path to the csv file with annotations.\n",
    "            sequence_max (int):\n",
    "            inner_dim (int):\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=MODEL_DIM, n_heads=NUM_HEADS, \n",
    "                 inner_dim=INNER_DIM, sequence_max=SEQUENCE_MAX):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.sequence_max = sequence_max\n",
    "        \n",
    "        self.multihead_attn = MultiheadAttention(embed_dim, n_heads)\n",
    "        self.feedforward = FFN(embed_dim, inner_dim, sequence_max)\n",
    "    \n",
    "    def temp_add_norm(self, x, y):\n",
    "        # TODO replace with layer norm function\n",
    "        x = np.add(x, y) \n",
    "        return (x / np.sqrt(np.sum(x**2))).T\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = self.multihead_attn(x)\n",
    "        x = self.temp_add_norm(x, y.T)\n",
    "        \n",
    "        y = self.feedforward(x)\n",
    "        x = self.temp_add_norm(x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#multihead_attn = MultiheadAttention()\n",
    "#ffn = FFN()\n",
    "#\n",
    "#x = dataset[0][\"input_embedding\"]\n",
    "#t = dataset[0][\"input_embedding\"]\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "#\n",
    "##multihead_attn.backprop(x, t)\n",
    "#\n",
    "#x = multihead_attn(x)\n",
    "\n",
    "\n",
    "#x = ffn(t.T)\n",
    "#\n",
    "#\n",
    "#ffn.backprop(x, t)\n",
    "#print(nn.loss(x, t))\n",
    "\n",
    "#for i in range(x.shape[1]):\n",
    "#    print(x[:,i].shape, t.T[:,i].shape)\n",
    "#    ffn.backprop(x[:,i], t.T[:,i])\n",
    "#    print(nn.loss(x, t.T))\n",
    "    \n",
    "\n",
    "#layer_norm(x)\n",
    "#enc_block = EncoderBlock()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
